\subsection{The Stochastic Representation by Gaussian Processes}
Let $s(t)$ for $t\in [0,\infty]$ be a continuous signal which is observed on discrete grid of points in the interval $[0,T]$. We consider not so uncommon situation, when the observed values of $s(t)$ are perturbed by some noise component. The perturbation of the true signal can be either deterministic (ie an chaotic system) or stochastic.  In fact, the process which we observe is the following
\begin{equation}\label{eq:signal_noisy_y}
y(t) = x(t) + \epsilon, \text{ for } \epsilon \in \mathcal{N}(0, \sigma^2).
\end{equation}
The signal $y(t)$ is observed at $t =( t_1 < \dots <t_N ) = \{ t_i \}_{i=1:N}$, where the subscripts represent the sampling index times. Therefore, our observation set consists of pairs $\big\{t_n,y_n\big\}$ where $y_n = y(t_n)$ for $t_n \in [0,T]$. 

For EMD to exists, the input signal needs to be approximated by a continuous representation; therefore, the discrete signal $s(t)$ is converted back into a continuous analog signal via a spline representation as in Equation \label{cubic_spl}. Having an continuous representation $S(t)$, which is an approximation of $s(t)$, we define the formulation of $S(t)$ given by EMD into $M$ intrinsic mode functions (IMFs) as follows 
\begin{equation}\label{eq:model_x_EMD}
S(t) = \sum_{m = 1}^M \gamma_m(t) + r(t) = \sum_{m = 1}^M \text{Re}\Big\{ A_m(t)  e^{i \theta_m(t)} \Big\} + r(t).
\end{equation}
where $r(t)$ represents a tendency which does not have much of oscillation and therefore characterize the low frequency tend of $S(t)$. The background on the EMD decomposition is given in Subsection \ref{ssec:EMD_background}.

\subsubsection{Continuous signal $S(t)$ as a Gaussian Process}
Out goal is to obtain a stochastic representation of the continuous signal $S(t)$ by a Gaussian Process. We postulate that each IMFs function, $\gamma_m(t)$, is a Gaussian process 
\begin{equation}\label{eq:model_IMF_GP_k}
\gamma_m(t) \sim \mathcal{GP} \Big(0, k_m(t,t')\Big), 
\end{equation}
where $k_m(t,t')$ is a positive definite covariance kernel which is parametrized by a set of parameters $\Psi_m$.  

Let us assume that we sample $S(t)$, and functions $\gamma_m(t)$ for $m = 1,\ldots, M$ at the $N$ time points $t_1 < \ldots <t_N$. We denote by $\mathbf{t}$ the vector of points $t_n$ for $n = 1,\ldots, N$.  

Therefore, given the observations $\gamma_m(\mathbf{t}) "= \big[ \gamma_m(t_1), \ldots, \gamma_m(t_N) \big]$, we would like to predict the values of $\gamma_m(t)$ at the argument $s$ that is $c_k(s)$, given the collected information in the observation set. Since $c(t)$ is a Gaussian Process, the random variable $\gamma_m(s)| \gamma_m(\mathbf{t}), \mathbf{t}$ is a Gaussian Process with the conditional mean
\begin{equation*}
\mu_m(s):=\mathbb{E}_{\gamma_m(t)|\gamma_m(\mathbf{t}), \mathbf{t}} \big[\gamma_m(s) \big] =  \mathbf{k}_m \big(s,\mathbf{t}\big) \mathbf{K}_m \big(\mathbf{t},\mathbf{t}\big)^{-1} \gamma_m(\mathbf{t})
\end{equation*}
and the conditional covariance matrix given by
\begin{equation*}
\tilde{k}_m(s,s'):= \mathbb{E}_{\gamma_m(t)|\gamma_m(\mathbf{t}), \mathbf{t}} \bigg[(\gamma_m(s) - \mu_m(s))(\gamma_m(s') - \mu_m(s'))\bigg] = k_m \big(s,s'\big) - \mathbf{k}_m\big(s,\mathbf{t}\big) \mathbf{K}_m \big(\mathbf{t},\mathbf{t}\big)^{-1} \mathbf{k}_m \big(\mathbf{t},s'\big) ^T
\end{equation*}
where
\begin{align*}
\mathbf{K}_m (\mathbf{t},\mathbf{t}) := \begin{bmatrix}
k_m(t_1,t_1) & k_m(t_1,t_2)& \cdots & k_m(t_1,t_{N}) \\
k_m(t_2,t_1) & k_m(t_2,t_2)& \cdots & k_m(t_2,t_{N}) \\
\vdots & \vdots & \ddots & \vdots  \\
k_m(t_{N}^{(i)},t_1) & k_m(t_{N},t_2)& \cdots & k_m(t_{N},t_{N}) 
\end{bmatrix}_{ N \times N}
\end{align*}
and
\begin{align*}
\mathbf{k}_m (s,\mathbf{t}) := \begin{bmatrix}
k_m(s,t_1) & k_m(s,t_2)& \cdots & k_m(s,t_{N})
\end{bmatrix}_{ 1 \times N}. 
\end{align*}

\paragraph{Multikernel Representation of $S(t)$}
The tendency component $r(t)$ can be modelled as a Gaussian Process itself or one can assume that $S(t)$ is a Gaussian Process conditioned on $r(t)$, that is
\begin{equation}\label{eq:xt_conditional_rep}
S(t) | r(t) \sim \mathcal{GP} \Big(r(t),  k(t,t')  \Big).
\end{equation}
where $k(t,t')$ is a function of the kernels $k_m(t,t')$ for $m \in \big\{1,\ldots,M \big\}$
These two approaches provide an unconditional and conditional stochastic representation of $S(t)$, respectively, and determine two different estimators of the out-of-sample forecast for $S(t)$.  The later is a more convenient assumption to preserve the monotonicity of the $r(t)$ which is a desired property of a residual function in the decomposition in Equation \eqref{eq:model_x_EMD}.  To ensure the function $r(t)$ to have only single convexity change, $r(t)$ might be extrapolated by a power law which stays monotonic (ie a polynomial up to the second order). Then, the out-of-sample forecast of $S(t)$ would be conditioned on the extrapolation of $r(t)$.  In order to preserve the monotonicity property of the tendency function $r(t)$ in the out-of-sample prediction, the extrapolation from a low order spline representation of $r(t)$, which is deterministic,  is excepted to behaves better than the forecast from a Gaussian Process since the later would most plausibly wiggle around a trend and, consequently, would loose the monotonicity of $r(t)$.  In the following work we would like to guarantee the out-of-sample monotonicity of $r(t)$ obtained by construction in the in-ample set,  and therefore, we chose to work with the conditional representation of $x(t)$ given in Equation \eqref{eq:xt_conditional_rep}.  {\color{red} TODO: derive the properties of these two estimators.}.


Given the Gaussian Process model of the $\gamma_m(t)$ in Equation \eqref{eq:model_IMF_GP_k}, the distribution of $S(t)$ can be formulated as a uniform mixture of Gaussian Processes with different kernels.  If we assume that the processes $\gamma_m(t)$ are independent, then, the stochastic representation of $S(t)$ from Equation \eqref{eq:xt_conditional_rep} can be formulated as follows
\begin{equation}
S(t)|r(t) \sim ~   GP \bigg(r(t); \sum_{m=1}^M k_m(t,t') \bigg) 
\end{equation}
If we denote by $k(t,t') := \sum_{m=1}^M k_m(t,t')$, then predictive distribution of $S(t)$ is given by 
\begin{equation}
\mu(s):= \mathbb{E}_{S(t)| r(t), \mathbf{s},\mathbf{t}} \big[S(\mathbf{s})] =  r(\mathbf{s}) + \sum_{m = 1}^M \mu_m(s)
\end{equation}
and the covariance matrix given by
\begin{equation}
\tilde{k}(s,s'):= \mathbb{E}_{S(t)|S(\mathbf{t}), \mathbf{t}} \bigg[(S_m(s) - \mu(s))(S(s') - \mu(s'))\bigg] = \sum_{m = 1}^M \tilde{k}(s,s') 
\end{equation}

If the processes of $\gamma_m(t)$ are not independent, the Gram matrix of the model for $S(t)$ contain additional elements which provide the correlation structure between different IMFs
\begin{equation}
s(t) |r(t)\sim ~   \mathcal{GP} \bigg(r(t); \sum_{m=1}^M k_m(t,t') + 2\sum_{m_1,m_2=1, m_1<m_2}^M k_{m1,m2}(t,t')\bigg) 
\end{equation}
where $k_{m1,m2}(t,t')$ defines the dependence structure between $\gamma_{m_1}(t)$ and $\gamma_{m_2}(t)$.






