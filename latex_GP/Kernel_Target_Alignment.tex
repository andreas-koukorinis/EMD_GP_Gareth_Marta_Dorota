\subsection{Kernel Target Alignment}

In this section we highlight an alternative method which estimates the Kernel matrix of the Gaussian Process of each IMF $\gamma_m(t)$, called Kernel Target Alignment (KTA) (\textcolor{red}{Cristianini et al., 2002}). There are two main encountered motivations behind such fact. The first one corresponding to a problem of identification of the Gaussian Process. It is well known (\textcolor{red}{reference - need to check}) that with Gaussian Processes, when it comes to several kernels (many of them - \textcolor{red}{gives examples}), the likelihood (or profile likelihood) of certain hyperparameters results to be a flat function (or very close to that) at the maximum. From a statistical perspective, this means that the model is not identifiable. Therefore, an identification challenge is found and there is an intuition for that: \textcolor{red}{TODO: examples to generate: take a finite amount of data from a Gaussian Process - just radial basis function, which has two parameters: length scale (variance term) and shape scale. The variance term or length scale, in the profile likelihood (for certain amount of data) is not different if you move from $t_1$ to $t_100$ or $t_1000$, and so on. It can be really really flat - and the packages that you download - they do MLE - they just hit the barrier - whatever is the truncation barrier of the parameter space is - and report that barrier}. Hence, the solution could be either the identification is forced, but this may be difficult if many kernels and, especially, complex kernels are employed, or an alternative procedure which does not suffer from identification issues is exploited. An example could be moment matching and the matrix version of it precisely corresponds to the Kernel Target Alignment (KTA). An important advantage of such method is that it is exact or unbiased since if you match exactly the second moment of a sample data and a Gaussian Process, you obtain a sufficient statistic (\textcolor{red}{TODO: we are not just capturing partial information - it is equivalent in the sense of sufficiency or unbiasedness - we don't know about consistency of minimum variance - we need to work on it }). The second motivation which justifies the employment of the KTA is the fact that it does not require the computation of the inverse of the Gram matrix; this operation corresponds to the order of $O(n^3)$ and can be highly expensive when dealing with big datasets. Moreover, the inversion is often numerically very unstable due to ill conditioning of the matrix.\\
In our context, we will consider realisations of a single IMF $\gamma_m(t)$ which is constructed through a smooth function; Gaussian Processes are realisations of functions which are smooth and therefore our population covariance matrix will be given by $\gamma_m(t) \gamma_m(t)'$ which is usually denoted in KTA frameworks as $yy'$. Our final objective will be identifying the set of hyperparameters for which the chosen kernel provides the moments which are as close as possible to the moments of the population model. This can be normally done through gradient based method (\textcolor{red}{see Gareth notes for reeferences - Nystrom method}); however, such techniques would require additional computational cost which are not needed in this settings since we will have few hyperparameters for each IMF to be estimated. The procedure will therefore as follows: for the Gaussian Process of each IMF we will select only one kernel (based on the in-sample learning \textcolor{red}{check with Gareth}). We then constraint maximum and minimum values of the hyperparameters of that kernel in a sensible way by building a grid. Afterwards, for each grid point, we evaluate the KTA and we choose the grid point which minimises the KTA. This will be the least difference between the population moments from the model and the sample model and the one of interest in our study. \\
Let us remark that the population covariance matrix in this case is given by $K_m (\bm{t}, \bm{t})$. In classification tasks, KTA consists of measuring the similarity between the ``ideal'' or target kernel and the one computed on the training set of the considered features. The alignment provides a measure for the degree of fitness of the given kernel. As given in \textcolor{red}{Cristianini et al.,}, the (empirical) alignment of a kernel $k_1$ with a kernel $k_2$ with respect to an (unlabelled) sample $S= \left\lbrace x_1, \dots, x_m \right\rbrace$ is given by:
\begin{equation}
\hat{A} \left(S, k_1, k_2  \right) = \frac{\left\langle K_1, K_2 \right\rangle_F}{\sqrt{\left\langle K_1, K_1 \right\rangle_F \left\langle K_2, K_2 \right\rangle_F}}
\end{equation}

where $\left\langle K_1, K_2 \right\rangle_F = \sum_{i,j = 1}^{m} K_1 (x_i,x_j) K_2 (x_i,x_j)$ represents the inner product between Gram matrices. If $K_2 = yy'$, where $y$ in our case corresponds to $\gamma_m(t)$, then the above equation becomes:

\begin{equation}
\hat{A} \left(S, K, \gamma_m(t) \gamma_m(t)'  \right) = \frac{\left\langle  K, \gamma_m(t) \gamma_m(t)' \right\rangle_F}{\sqrt{\left\langle K, K \right\rangle_F \left\langle \gamma_m(t) \gamma_m(t)', \gamma_m(t) \gamma_m(t)' \right\rangle_F}} = \frac{\left\langle  K, \gamma_m(t) \gamma_m(t)' \right\rangle_F}{m \sqrt{\left\langle  K, K' \right\rangle_F}}, \quad \mbox{since} \quad \left\langle  \gamma_m(t) \gamma_m(t)', \gamma_m(t) \gamma_m(t)' \right\rangle_F = m^2
\end{equation}

Cristianini et al. proved that $\hat{A}$ gives a reliable estimate of its expected value by being concentrated around its mean. \\
\textcolor{red}{TODO: add the part on CA which is the centered alignment which is required so that the moments of population model and sample covariance are actually true - we need to preprocess our GP (detrending it) or applying CA and not KTA. - I would prefer first option.\\
TODO: for next week - some example of this on R.}